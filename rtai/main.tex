\documentclass[11pt]{article}
\usepackage[margin,header,sans,titlepage,bib]{../lecture_notes}


\title{Reliable and Trustworthy AI}
\author{Saurav Banka}
\semester{HS 2025}
\lecturer{Dr. Martin Vechev}
\date{\today}
\addbibresource{main.bib}

\begin{document}
\maketitle

\section{Lecture 1: Introduction (17.09.25)}
\subsection{Motivation} % (fold)
\label{sub:motivation}
Traditional ML progress focused on \textbf{standard accuracy} (e.g., ImageNet).  
Deployments in real-world settings often reveal failures:
\begin{itemize}
    \item Distribution shifts $\rightarrow$ performance drop.
    \item Safety-critical failures (autonomous driving, incorrect medical diagnoses).
\end{itemize} 

When deploying models into the real world, we need to know that models are safe, secure, robust, transparent and reliable. The goal of this course is to get a fundamental understanding of privacy and robustness techniques, and a glimpse into the latest research into reliable and trustworthy AI.
% subsection motivation (end)

\subsection{Vertical I: Robustness}
\label{sub:robustness_intro}
Robustness research broadly tackles questions:
\begin{itemize}
	\item What attacks (e.g.\ gradient-based, branch-and-bound) and defenses exist?
	\item Is it possible to certify / prove robustness and performance under perturbations or adversaries?
	\item How to train models that are provably robust?
\end{itemize}

\subsubsection{Why is certifying robustness difficult?}
Consider the use-case of image classification (e.g.\ MRI images) under input perturbations (noise, blur, rotation) in a range that does not affect human ground-truth labels.  
The decision boundary of a neural network is a $(d-1)$-dimensional hypersurface in the input space $\RR^d$.  
The goal is that the model prediction should remain invariant to such perturbations.

Certifying robustness involves:
\begin{enumerate}
	\item \textbf{Precondition $\varphi(x)$:} convex polytope of all possible perturbations of $x$.  
	      \emph{Problem:} prohibitively large to enumerate, especially in higher dimensions or under multiple transformations.
	\item \textbf{Propagation:} push this region through the network layers.  
	      \emph{Problem:} results in many small non-convex shapes. Exact methods (MILP, SMT) are NP-complete and do not scale.
	\item \textbf{Abstraction:} use convex relaxations to approximate the shapes with an enclosing convex polytope.  
	      If the final convex region (post-condition) lies entirely in the correct class, the model is robust.  
	      \emph{Problem:} loose relaxations introduce false positives and reduce provability.
\end{enumerate}

\textbf{Summary:} Tight approximations are precise but expensive; loose approximations are efficient but may miss guarantees.

\subsubsection{Training Certified Models}
Even if robustness could be proven efficiently, unless a network is trained to be provable, it is unlikely to satisfy such specifications.

\textbf{Key idea:} Instead of propagating individual datapoints, propagate convex regions (symbolic inputs).  
Backpropagation then uses symbolic information to reduce the size of post-conditions, encouraging perturbations to cluster closely in representation space.

\paragraph{Objective.}  
Standard training minimizes
\[
    \min_\theta \; \EE \big[ \loss(\theta, x, y) \big].
\]
With a robustness specification, this becomes a min–max optimization:
\[
    \min_\theta \; \EE \Big[ \max_{x' \in \varphi(x)} \loss_{(\varphi, \psi)}(\theta, x', y) \Big].
\]
Interpretation: minimize the worst-case loss over perturbations.  
\emph{Challenge:} This problem is much harder to optimize, may fail to enforce specifications, and often degrades standard accuracy.

\subsubsection{Individual Fairness and Randomized Smoothing}
\textbf{Individual fairness:} If two datapoints are similar in relevant aspects for a task, they should receive similar predictions.  
This is closely related to robustness: perturbations in sensitive attributes should not flip outputs.

\textbf{Randomized smoothing:} An inference-time defense that replaces the classifier with a smoothed version.  
Given an input $x$, add Gaussian noise and average predictions.  
Guarantee: nearby inputs map to the same label with high probability.  
Compared to certified training, randomized smoothing scales better but only provides guarantees for certain robustness properties.


\subsection{Vertical II: Privacy}
\label{sub:privacy_intro}
Privacy in this course is primarily focused on \textbf{data privacy}: preventing leakage of sensitive information from models. Key research questions:

\begin{itemize}
	\item \textbf{Membership inference:} Given a datapoint $x$, can an attacker determine if $x$ was in the training set?
	\item \textbf{Model inversion:} Can an attacker reconstruct a representative example from a given class?
	\item \textbf{Training data extraction:} Can raw training samples be recovered from a model (e.g.\ LLMs regurgitating text)?
	\item \textbf{Private attribute inference:} Can sensitive attributes (age, gender, location) be inferred from user data?  
	\item \textbf{Model stealing:} Recovering model weights or functionality from black-box access.
\end{itemize}

Defenses:
\begin{itemize}
	\item \textbf{Differential Privacy (DP):} Add noise during training (e.g.\ DP-SGD).  
	      Guarantees: presence/absence of one sample does not significantly affect the output distribution.  
	      Trade-off: higher privacy $\leftrightarrow$ lower accuracy. Recent work shows DP-LLMs approaching performance of earlier non-private models.
	\item \textbf{Federated Learning:} Train across distributed devices (e.g.\ smartphones) without centralizing data.  
	      Related to fine-tuning multiple LLMs and merging them securely.
\end{itemize}

Beyond standalone models, in compound systems such as \textbf{agentic AI}, LLMs are integrated with external tools. Safety risks emerge from multi-step workflows (e.g.\ insecure protocols, toxic flows). Ensuring provable safety here remains an open challenge.

% subsection privacy (end)

\subsection{Vertical III: Provenance and Evaluation}
\label{sub:provenance_intro}
Two central themes in this vertical are \textbf{data attribution} (who generated what) and \textbf{evaluation} (how to measure performance fairly).

\subsubsection{Watermarking and Data Attribution}
\begin{itemize}
	\item \textbf{Idea:} Randomly assign tokens a color (expect evenly distributed) and transform the output distribution to skew to a color.
	\item \textbf{Threats:}
		\begin{itemize}
			\item \emph{Stealing:} Approximate the watermark distribution with repeated queries.
			\item \emph{Spoofing:} Generate text that falsely appears to come from a target model.
			\item \emph{Scrubbing:} Remove watermark to conceal that text was model-generated.
		\end{itemize}
\end{itemize}

\subsubsection{Transformations and Safety}
Common LLM transformations (quantization, pruning, distillation, fine-tuning) may impact safety and privacy guarantees.  

\emph{Example:} A model appears normal pre-quantization in benchmarks, etc but after quantization begins inserting hidden adversarial content (e.g.\ advertisements).

\subsubsection{Evaluation and Benchmarks}
\begin{itemize}
	\item \textbf{Benchmarks:}
	      \begin{itemize}
		      \item Closed-form (e.g.\ math problems with unique solutions, math arena).
		      \item Preference-based (e.g.\ LLM Arena where users vote).
	      \end{itemize}
	\item \textbf{Contamination:} Training/test set overlap or task leakage.  
	      Leads to inflated benchmark results.
	\item \textbf{Real-world failures:} Some model releases (e.g.\ K2) were later shown to have flawed evaluations due to contamination or misleading comparisons.
\end{itemize}
\newpage
% subsection provenance (end)
%%%%%%%%%%%%%%%%% END OF LECTURE 1. %%%%%%%%%%%%%%%%% 

\section{Lecture 2: Adversarial Attacks and Defenses (24.09.2025)} % (fold)
\subsection{Examples}
\begin{itemize}
	\item Noisy attacks and perturbations: add imperceptible noise that does not affect humans but changes the label. Also applicable in other domains such as reinforcement learning, NLP, and audio-to-text.
	\item Physical attacks: adding tape or patches to stop signs changes predictions.
	\item In systems: patches placed on camera inputs can cause self-driving cars to make incorrect decisions.
	\item Geometric perturbations: rotations, translations, etc. This is an interesting class of attacks since it lies in a lower-dimensional space (e.g. degrees of rotation vs. high-dimensional pixel space) and does not necessarily require access to gradients.
\end{itemize}

\subsection{Adversarial Attacks}
Types of attacks:
\begin{itemize}
	\item \textbf{Targeted attack}: Misclassify input to a specific label, or get an LLM to generate a specific text. 
	\item \textbf{Untargeted attack}: Misclassify input to any wrong label. These tend to capture the worst case, and optimization problems usually utilize this in adversarial defenses.
\end{itemize}

Information known to the adversary:
\begin{itemize}
	\item \textbf{White-box attacks}: Attacker knows the model, parameters, architecture (essentially everything).
	\item \textbf{Black-box attacks}: Attacker does not know the parameters or weights, but may have query access to predictions or logits.  
\end{itemize}
Adversarial attacks are transferable: an attacker can train their own surrogate network (e.g. via distillation) and use white-box techniques to generate adversarial examples that often transfer with high success rate to black-box models. (Note: do proofs for certain properties also transfer? In some cases, yes, under specific conditions.)

\subsubsection{Targeted attack – Targeted FGSM}
\textbf{Setup}: Given a neural network $f: X \to C$, input $x \in X$, and target label $t \in C$ such that $f(x) \neq t$, the goal is to find a perturbation $\eta$ such that $f(x + \eta) = t$.

One way to perform this attack is using Targeted FGSM, a fast one-step method. $\epsilon$ is a small constant controlling the perturbation. FGSM ensures that $\| \eta \|_\infty \leq \epsilon$, i.e. each pixel lies in $[x_i - \epsilon,\, x_i + \epsilon]$.

\begin{algorithm}
\caption{Targeted Fast Gradient Sign Method}
\begin{algorithmic}[1]
\State Compute the perturbation: $\eta = \epsilon \cdot \operatorname{sign}(\nabla_x L(x, t))$
\State Perturb the input: $x' = x - \eta$
\State Check if $f(x') = t$ 
\end{algorithmic}
\end{algorithm}

\begin{note}
	Intuition: perturb the input to reduce the loss for classifying the image as label $t$. No guarantees on perturbation magnitude — it may still be noticeable.
\end{note}

\subsubsection{Untargeted attack}
Given a neural network $f: X \to C$ and input $x \in X$, find a perturbation $\eta$ such that $f(x + \eta) \neq f(x)$.

\begin{algorithm}
\caption{Untargeted Fast Gradient Sign Method}
\begin{algorithmic}[1]
\State Compute the perturbation: $\eta = \epsilon \cdot \operatorname{sign}(\nabla_x L(x, s))$ \Comment{$s = f(x)$ is the current label}
\State Perturb the input: $x' = x + \eta$
\State Check if $f(x') \neq s$ 
\end{algorithmic}
\end{algorithm}

\begin{note}
	Intuition: maximize the loss for the current label $s$ to push the example away from its original classification.
\end{note}

\subsubsection{Targeted attack with small changes – Carlini \& Wagner}
To ensure the perturbed image is similar to the original, we constrain the perturbation norm (e.g. $\ell_\infty$ norm).

\textbf{Setup}: Same as targeted attack, but minimize $\|\eta\|_p$ subject to $f(x+\eta) = t$ and $x + \eta \in [0,1]^n$.

Formulated as:
\[
  \optprob{Find $\eta^*$}{\min}{\|\eta\|_p}[%
    f(x+\eta) = t \\ x + \eta \in [0, 1]^n
  ]
\]

\textbf{Issue 1:} The discrete constraint $f(x+\eta) = t$ is hard to optimize. Relaxation (Carlini \& Wagner \cite{carlini2017towards}):
\begin{enumerate}
	\item Define a proxy objective $obj_t$ such that $obj_t(x+\eta) \leq 0 \implies f(x+\eta) = t$.
	\item Optimize:
	\[
	  \optprob{Find $\eta^*$}{\min}{\|\eta\|_p + c \cdot obj_t(x+\eta)}[%
	    \eta \in [0, 1]^n
	  ]
	\]
\end{enumerate} 

\begin{table}[h!]
\centering
\renewcommand{\arraystretch}{1.4}
\begin{tabular}{|c|c|}
\hline
\textbf{Objective function} & \textbf{Explanation} \\
\hline
$\displaystyle -\log p(t) - 1$ &
\begin{minipage}[t]{0.65\textwidth}
\begin{itemize}[leftmargin=*]
    \item If $obj_t(x) \le 0$, then $p(t) \ge 0.5$ (binary case).
    \item If $p(t) \ge 0.5$, then $f$ will classify as $t$.
    \item Smooth and decreases as $p(t)$ increases, encouraging reclassification.
\end{itemize}
\end{minipage} \\
\hline
$\displaystyle \max\!\bigl(0,\,0.5 - p(t)\bigr)$ &
\begin{minipage}[t]{0.65\textwidth}
\begin{itemize}[leftmargin=*]
    \item If $obj_t(x) \le 0$, then $p(t) \ge 0.5$.
    \item Penalizes when $p(t) < 0.5$, pushing toward target.
    \item Stops pushing once $p(t) \ge 0.5$, enforcing minimal change.
\end{itemize}
\end{minipage} \\
\hline
\end{tabular}
\caption{Examples of CW-style objectives. In practice, logits are often used instead of probabilities.}
\end{table}

% (retain your notes about Issue 2, Issue 3, PGD, etc. – unchanged except fixed typos/notation)

\subsection{Defenses}
\textbf{Adversarial accuracy}: When measuring accuracy, generate adversarial examples for correctly classified test points. For example, if 95/100 are correct, and 15 of these are broken by adversarial examples, then adversarial accuracy = 80\%. Increasing adversarial accuracy usually decreases standard accuracy. 

Defense optimization:
\[
	\min_\theta \; \EE_{(x, y) \sim D}\left[ \max_{x' \in S(x)} L(\theta, x', y) \right]
\]

Where $S(x) = \{x' \in \RR^n \mid \|x - x'\|_p \leq \epsilon\}$ is the perturbation region. 

\begin{algorithm}
\caption{Adversarial training using PGD}
\begin{algorithmic}[1]
\State Select minibatch $B$ from dataset $D$.
\For{$(x, y) \in B$}
	\State Find $x_{max} = \argmax_{x' \in S(x)} L(\theta, x', y)$ (e.g. via PGD)
\EndFor
\State Update parameters: $\theta \gets \theta - \frac{1}{|B_{max}|}\sum_{(x_{max}, y) \in B_{max}} \nabla_{\theta} L(\theta, x_{max}, y)$
\end{algorithmic}
\end{algorithm}

Larger models are generally more robust to adversarial training. On small models, adversarial training can hurt clean accuracy. In practice, PGD-based training yields better robustness than FGSM-based training.

\newpage
\subsection{Lecture 3: }

\newpage
\printbibliography

\end{document}