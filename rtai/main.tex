\documentclass[11pt]{article}
\usepackage[margin,header,sans,titlepage,bib]{../lecture_notes}


\title{Reliable and Trustworthy AI}
\author{Saurav Banka}
\semester{HS 2025}
\lecturer{Dr. Martin Vechev}
\date{\today}
\addbibresource{main.bib}

\begin{document}
\maketitle

\section{Lecture 1: Introduction (17.09.25)}
\subsection{Motivation} % (fold)
\label{sub:motivation}
Traditional ML progress focused on \textbf{standard accuracy} (e.g., ImageNet).  
Deployments in real-world settings often reveal failures:
\begin{itemize}
    \item Distribution shifts $\rightarrow$ performance drop.
    \item Safety-critical failures (autonomous driving, incorrect medical diagnoses).
\end{itemize} 

When deploying models into the real world, we need to know that models are safe, secure, robust, transparent and reliable. The goal of this course is to get a fundamental understanding of privacy and robustness techniques, and a glimpse into the latest research into reliable and trustworthy AI.
% subsection motivation (end)

\subsection{Vertical I: Robustness}
\label{sub:robustness_intro}
Robustness research broadly tackles questions:
\begin{itemize}
	\item What attacks (e.g.\ gradient-based, branch-and-bound) and defenses exist?
	\item Is it possible to certify / prove robustness and performance under perturbations or adversaries?
	\item How to train models that are provably robust?
\end{itemize}

\subsubsection{Why is certifying robustness difficult?}
Consider the use-case of image classification (e.g.\ MRI images) under input perturbations (noise, blur, rotation) in a range that does not affect human ground-truth labels.  
The decision boundary of a neural network is a $(d-1)$-dimensional hypersurface in the input space $\RR^d$.  
The goal is that the model prediction should remain invariant to such perturbations.

Certifying robustness involves:
\begin{enumerate}
	\item \textbf{Precondition $\varphi(x)$:} convex polytope of all possible perturbations of $x$.  
	      \emph{Problem:} prohibitively large to enumerate, especially in higher dimensions or under multiple transformations.
	\item \textbf{Propagation:} push this region through the network layers.  
	      \emph{Problem:} results in many small non-convex shapes. Exact methods (MILP, SMT) are NP-complete and do not scale.
	\item \textbf{Abstraction:} use convex relaxations to approximate the shapes with an enclosing convex polytope.  
	      If the final convex region (post-condition) lies entirely in the correct class, the model is robust.  
	      \emph{Problem:} loose relaxations introduce false positives and reduce provability.
\end{enumerate}

\textbf{Summary:} Tight approximations are precise but expensive; loose approximations are efficient but may miss guarantees.

\subsubsection{Training Certified Models}
Even if robustness could be proven efficiently, unless a network is trained to be provable, it is unlikely to satisfy such specifications.

\textbf{Key idea:} Instead of propagating individual datapoints, propagate convex regions (symbolic inputs).  
Backpropagation then uses symbolic information to reduce the size of post-conditions, encouraging perturbations to cluster closely in representation space.

\paragraph{Objective.}  
Standard training minimizes
\[
    \min_\theta \; \EE \big[ \loss(\theta, x, y) \big].
\]
With a robustness specification, this becomes a min–max optimization:
\[
    \min_\theta \; \EE \Big[ \max_{x' \in \varphi(x)} \loss_{(\varphi, \psi)}(\theta, x', y) \Big].
\]
Interpretation: minimize the worst-case loss over perturbations.  
\emph{Challenge:} This problem is much harder to optimize, may fail to enforce specifications, and often degrades standard accuracy.

\subsubsection{Individual Fairness and Randomized Smoothing}
\textbf{Individual fairness:} If two datapoints are similar in relevant aspects for a task, they should receive similar predictions.  
This is closely related to robustness: perturbations in sensitive attributes should not flip outputs.

\textbf{Randomized smoothing:} An inference-time defense that replaces the classifier with a smoothed version.  
Given an input $x$, add Gaussian noise and average predictions.  
Guarantee: nearby inputs map to the same label with high probability.  
Compared to certified training, randomized smoothing scales better but only provides guarantees for certain robustness properties.


\subsection{Vertical II: Privacy}
\label{sub:privacy_intro}
Privacy in this course is primarily focused on \textbf{data privacy}: preventing leakage of sensitive information from models. Key research questions:

\begin{itemize}
	\item \textbf{Membership inference:} Given a datapoint $x$, can an attacker determine if $x$ was in the training set?
	\item \textbf{Model inversion:} Can an attacker reconstruct a representative example from a given class?
	\item \textbf{Training data extraction:} Can raw training samples be recovered from a model (e.g.\ LLMs regurgitating text)?
	\item \textbf{Private attribute inference:} Can sensitive attributes (age, gender, location) be inferred from user data?  
	\item \textbf{Model stealing:} Recovering model weights or functionality from black-box access.
\end{itemize}

Defenses:
\begin{itemize}
	\item \textbf{Differential Privacy (DP):} Add noise during training (e.g.\ DP-SGD).  
	      Guarantees: presence/absence of one sample does not significantly affect the output distribution.  
	      Trade-off: higher privacy $\leftrightarrow$ lower accuracy. Recent work shows DP-LLMs approaching performance of earlier non-private models.
	\item \textbf{Federated Learning:} Train across distributed devices (e.g.\ smartphones) without centralizing data.  
	      Related to fine-tuning multiple LLMs and merging them securely.
\end{itemize}

Beyond standalone models, in compound systems such as \textbf{agentic AI}, LLMs are integrated with external tools. Safety risks emerge from multi-step workflows (e.g.\ insecure protocols, toxic flows). Ensuring provable safety here remains an open challenge.

% subsection privacy (end)

\subsection{Vertical III: Provenance and Evaluation}
\label{sub:provenance_intro}
Two central themes in this vertical are \textbf{data attribution} (who generated what) and \textbf{evaluation} (how to measure performance fairly).

\subsubsection{Watermarking and Data Attribution}
\begin{itemize}
	\item \textbf{Idea:} Randomly assign tokens a color (expect evenly distributed) and transform the output distribution to skew to a color.
	\item \textbf{Threats:}
		\begin{itemize}
			\item \emph{Stealing:} Approximate the watermark distribution with repeated queries.
			\item \emph{Spoofing:} Generate text that falsely appears to come from a target model.
			\item \emph{Scrubbing:} Remove watermark to conceal that text was model-generated.
		\end{itemize}
\end{itemize}

\subsubsection{Transformations and Safety}
Common LLM transformations (quantization, pruning, distillation, fine-tuning) may impact safety and privacy guarantees.  

\emph{Example:} A model appears normal pre-quantization in benchmarks, etc but after quantization begins inserting hidden adversarial content (e.g.\ advertisements).

\subsubsection{Evaluation and Benchmarks}
\begin{itemize}
	\item \textbf{Benchmarks:}
	      \begin{itemize}
		      \item Closed-form (e.g.\ math problems with unique solutions, math arena).
		      \item Preference-based (e.g.\ LLM Arena where users vote).
	      \end{itemize}
	\item \textbf{Contamination:} Training/test set overlap or task leakage.  
	      Leads to inflated benchmark results.
	\item \textbf{Real-world failures:} Some model releases (e.g.\ K2) were later shown to have flawed evaluations due to contamination or misleading comparisons.
\end{itemize}
\newpage
% subsection provenance (end)
%%%%%%%%%%%%%%%%% END OF LECTURE 1. %%%%%%%%%%%%%%%%% 

\section{Lecture 2: Adversarial Attacks and Defenses (24.09.2025)} % (fold)
\subsection{Examples}
\begin{itemize}
	\item Noisy attacks and perturbations: add imperceptible noise to humans, changes label. Also applicable in other domains like reinforcement learning paradigm, NLP and audio-to-text.
	\item Physical attacks: Tape on stop signs changes prediction
	\item In systems: Missing/patches in immages causes self-driving cars
	\item Geometric pertubations: rotations / translations, etc. This is interesting class of attacks as it is in a lower-dimensional space (e.g. degree of rotation vs higher-dimension pixel space) and doesn't need access to gradients
\end{itemize}

\subsection{Adversarial Attacks}
Types of attacks:
\begin{itemize}
	\item \textbf{Targeted attack}: Misclassify input to a specific label, or get an LLM to generate a specific text. 
	\item \textbf{Untargeted attack}: Misclassify input to any wrong label. These tend to capture the worst case, and optimization problems usually utilize this in adversarial defences.
\end{itemize}

Information known to the adversary:
\begin{itemize}
	\item \textbf{White box attacks}: Attacker know the model, parameters, model architecture (essentially everything)
	\item \textbf{Black box attacks}: Attacker knows weights but not the parameters, but maybe the logits.  
\end{itemize}
Adversarial attacks are transferrable: an attacker can train their own mirror network (e.g. distill model, etc) and use white-box techniques on the black-box network. This works with high success rate. (Note: do proofs for certain properties also transfer? Turns out that they transfer under certain specifications).


\subsubsection{Targeted attack - Targeted FGSM}
\textbf{Setup}: Given an a neural network $f: X \Rightarrow C$, input $x \in X$, target label $t \in C$ such that $f(x) \neq t$, the output should be a perturbation $\eta$ s.t $f(x + \eta) = t$.

One way to perform this attack is using Targeted FGSM, which is designed to be fast / one-step, $\epsilon$ is a small constant part of the spec. Since FGSM is 1-step, $x'$ guaranteed to stay in box $[x - \epsilon, x + \epsilon]$

\begin{algorithm}
\caption{Targeted Fast Gradient Sign Method}
\begin{algorithmic}[1]
\State Compute the perturbation: $\eta = \epsilon * sign(\nabla_x loss_t(x))$
\State Perturb the input: $x' = x - \eta$
\State Check if $f(x') = t$ 
\end{algorithmic}
\end{algorithm}

\begin{note}
	Intuition: Perturb the input to reduce the loss of classifying the image as label $t$. No guarantees on magnitude of $t$ which may make perturnations too noticable.
\end{note}

\subsubsection{Untargeted attack}
Given an a neural network $f: X \Rightarrow C$, input $x \in X$, the output should be a perturbation $\eta$ s.t $f(x + \eta) \neq f(x)$.

\begin{algorithm}
\caption{Untargeted Fast Gradient Sign Method}
\begin{algorithmic}[1]
\State Compute the perturbation: $\eta = \epsilon * sign(\nabla_x loss_s(x))$
\State Perturb the input: $x' = x _ \eta$
\State Check if $f(x') \neq s$ 
\end{algorithmic}
\end{algorithm}

\begin{note}
	Intuition: Find perturbations to try to "get away" from current label $s$ by maximizing the value of the loss. 
\end{note}

\subsubsection{Targeted attack with small changes - Carlini and Wagner}
To ensure that the perturbed image is similar to the initial image, we need a notion of distance between the input and perturbed images (e.g. $l_\infty$ norm which captures the maximum noise change). 

\textbf{Setup}: Same setup as target attack, with the additional constraint that $\norm{\eta}_p$ is minimized. Set up an optimization problem to find $\eta$

This is an optimization problem defined as follows:
\[
  \optprob{Find $\eta^*$}{\min}{\norm{\eta}_p}[%
    f(x+\eta) = t \\ x + \eta \in [0, 1]^n
  ]
\]

\textbf{Issue 1}: The first issue is that $f(x + \eta) = t$ is a hard discrete constraint which is hard to optimize. The key insight is to relax this to a soft optimization problem as defined in the paper \cite{carlini2017towards}. Two steps:
\begin{enumerate}
	\item Define a proxy function $obj_t$ such that if $obj_t(x + \eta) \leq 0$ then $f(x + \eta) = t$
	\item Solve the following optimization problem:
	\[
	  \optprob{Find $\eta^*$}{\min}{\norm{\eta}_p + c \cdot obj_t(x + \eta)}[%
	    \eta \in [0, 1]^n
	  ]
	\]
\end{enumerate} 

\begin{table}[h!]
\centering
\renewcommand{\arraystretch}{1.4}
\begin{tabular}{|c|c|}
\hline
\textbf{Objective function} & \textbf{Explanation and why it works} \\
\hline
$\displaystyle -\log_2 p(t) - 1$ &
\begin{minipage}[t]{0.65\textwidth}
\begin{itemize}[leftmargin=*]
    \item If $obj_t(x) \le 0$, then $p(t) \ge 0.5$.
    \item But if $p(t) \ge 0.5$ for the input $x$, then $f$ will return $t$ as a classification for $x$ because this is the highest probability class.
    \item Smooth and decreases as $p(t)$ increases, so it strongly encourages reclassification, especially when $p(t)$ is small.
\end{itemize}
\end{minipage} \\
\hline
$\displaystyle \max\!\bigl(0,\,0.5 - p(t)\bigr)$ &
\begin{minipage}[t]{0.65\textwidth}
\begin{itemize}[leftmargin=*]
    \item If $obj_t(x) \le 0$, then $p(t) \ge 0.5$.
    \item Same as above, $f$ will return $t$ as a classification for $x$ because this is the highest probability class.
    \item Penalizes only when $p(t) < 0.5$, pushing toward reclassification.
    \item Stops pushing once $p(t) \ge 0.5$, enforcing only the minimum change needed.
\end{itemize}
\end{minipage} \\
\hline
\end{tabular}
\caption{Examples of CW-style objective functions. Both satisfy the Step~1 property:
if $obj_t(x + \eta) \le 0$ then $f(x + \eta)=t$.} 
\end{table}

\begin{exercise}
	Read through C\&W paper \cite{carlini2017towards} and work through why the property in step 1 holds and the intuition behind the other 6 objectives mentioned in addition to the two above.
\end{exercise}

\textbf{Issue 2}: $\norm{\eta}_\infty$ is hard to optimize. 
\begin{note}
The following content wasn't covered in lecture, but linked as a Youtube video. \\ \\
The $\ell_\infty$ norm measures the maximum change across all coordinates:
\[
\norm{\eta}_\infty = \max_i |\eta_i|.
\]
This is a non-smooth function: the gradient is nonzero only at the coordinates that currently achieve the maximum value, and is zero everywhere else. As a result, gradient descent will only update one coordinate at a time (the maximum one). Additionally, since this is not the only objective being optimized due to constraints, the components decreased in one iteration may be increased again in a subsequent one, leading to oscillations and very slow convergence. This makes direct optimization of $\norm{\eta}_\infty$ impractical. 

To overcome this, replace $\norm{\eta}_\infty$ with a proxy:
\[
L(\eta) = \sum_i \max(0,\, |\eta_i| - \tau),
\]
where $\tau$ is gradually decreased during optimization. The gradient of this proxy is
\[
\frac{\partial L}{\partial \eta_i} =
\begin{cases}
\operatorname{sign}(\eta_i), & |\eta_i| > \tau, \\
0, & |\eta_i| \leq \tau,
\end{cases}
\]
which means that all coordinates exceeding $\tau$ are penalized simultaneously, spreading the optimization effort instead of focusing only on the single maximum.

When optimization finishes, the smallest feasible $\tau$ corresponds exactly to the $\ell_\infty$ bound on $\eta$. Thus, minimizing the proxy objective yields the same solution as minimizing $\norm{\eta}_\infty$, but in a way that is smooth, well-behaved, and more suitable for gradient-based methods.
\end{note}

\textbf{Issue 3}: Dealing with box constraints.  

When we perturb the input $x$, the perturbed version $x+\eta$ must remain a valid image, i.e.\ each pixel must lie within $[0,1]$. This imposes a \emph{box constraint}:
\[
x+\eta \in [0,1]^n \quad \Longleftrightarrow \quad \eta_i \in [-x_i,\, 1-x_i] \;\; \text{for all } i.
\]

To keep the image within the box constraint, two potential approaches:
\begin{enumerate}
\item\textbf{Projected gradient descent (PGD)} handles this by projecting back into the box after each update:
\[
project(\eta) = \bigl(clip_1(\eta_1), \dots, clip_n(\eta_n)\bigr),
\]
where 
\[
clip_i(\eta_i) =
\begin{cases}
-x_i, & \eta_i < -x_i, \\
\eta_i, & \eta_i \in [-x_i, 1-x_i], \\
1-x_i, & \eta_i > 1-x_i.
\end{cases}
\]
This ensures every update keeps $x+\eta$ within the valid range.

\item\textbf{Carlini--Wagner} instead use the L-BFGS-B optimizer, a quasi-Newton method that directly supports box constraints. At a high level, L-BFGS-B:
\begin{itemize}[leftmargin=*]
    \item Approximates second-order information (curvature) using limited memory, making it more efficient than full Newton’s method.  
    \item Incorporates the box constraints $\eta_i \in [-x_i,\,1-x_i]$ directly into the optimization procedure.  
    \item Ensures that every step taken respects the bounds, so no explicit projection is needed afterward.  
\end{itemize}
\end{enumerate}

In both cases, the goal is the same: enforce the constraint that perturbed images remain valid inputs while searching for adversarial examples.

\subsubsection{Projected Gradient Descent (PGD) attack}

PGD is an iterative untargeted attack that searches the allowed perturbation region for a point that \emph{maximizes} the loss (i.e. likely to cause misclassification). Starting from a random point inside the allowed box to avoid always following the same gradient trajectory, take several small FGSM-like steps. After each FGSM step we \emph{project} back to the feasible set so the perturbed image remains valid.

\begin{note}
	\textbf{Intuition:} 
	\begin{itemize}[leftmargin=*]
	  \item PGD seeks a point \emph{inside the allowed box} that yields large loss (and therefore likely a different label).
	  \item Randomizing the start avoids always searching from the same corner of the box and helps find stronger adversarial points.
	  \item Using many small FGSM-like steps explores the box more carefully than a single big step.
	  \item Projecting after each step guarantees $x^{(t)}$ remains a valid image and stays within the allowed perturbation radius.
	\end{itemize}
\end{note}

\begin{note}
    \textbf{Projections:}
    \begin{itemize}[leftmargin=*]
      \item For $\ell_\infty$ constraints, projection is cheap: each pixel is simply clipped coordinate-wise to $[x_i-\varepsilon,\,x_i+\varepsilon]\cap[0,1]$. 
      \item Cheap projections are essential since PGD projects at every step. If projection were expensive, the attack would be too impractical or too expensive to use in adversarial pre-training.
      \item In contrast, projecting onto general convex polyhedra can be computationally hard, and finding efficient methods is an open problem.
    \end{itemize}
\end{note}

\subsection{Defenses}
\textbf{Adversarial accuracy}: When measuring the accuracy of a model, we generate adversarial examples on correctly classified examples in the test set.  (E.g. 95/100 examples classified correctly, 15 adversarial examples found i.e. 80\% adversarial accuracy). Raising the adversarial accuracy often hurts the overall test accuracy. 

Defense can be viewed as the following optimization problem:
\[
	\min_\theta \EE_{(x, y) \sim D}\left[ \max_{x' \in S(x)} L(\theta, x, y) \right]
\]

Where $D$ is the underlying distribution, $S(x) = \{x' \in \RR^n | \norm{x - x'}_p \leq \epsilon \}$ is the perturbation region. Intuitively, we're minimizing the empirical risk of the worst case behavior in the perturbation region.

In practice, this is done as follows:

\begin{algorithm}
\caption{Adversarial training using PGD}
\begin{algorithmic}[1]
\State Select minibatch $B$ from dataset $D$.
\For{$(x, y) \in B$}
	\State Find $x_{max} = \argmax_{x' \in S(x)}{L(\theta, x', y)}$ (e.g. using PGD)
\EndFor
\State Solve outer optimization problem: $\theta \gets $\theta - \frac{1}{|B_{max}|}\sum_{(x_{max}, y) \in B_{max}} \nabla_{\theta} L(\theta, x_{max}, y)$
\end{algorithmic}
\end{algorithm}

Model capacity is important - larger networks are more defendable, whereas training smaller networks with PGD negatively affects accuracy. On large networks, accuracy on test set may still suffer due to adversarial pretraining. Usually, training with adversarial examples from PGD attacks performs better than training with adversarial examples using FGSM.

\newpage
\subsection{Lecture 3: }

\newpage
\printbibliography

\end{document}